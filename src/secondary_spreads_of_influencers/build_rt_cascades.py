#!/usr/bin/env python3

import argparse
import heapq
import networkx as nx
from tqdm import tqdm
import polars as pl
from models import Tweet
import json


arg_parser = argparse.ArgumentParser()
arg_parser.add_argument("--save_path", type=str, required=True)
arg_parser.add_argument("--collected_retweets_path", type=str, required=True)
arg_parser.add_argument("--qt_ids_path", type=str, required=True)
arg_parser.add_argument("--follow_relation_data_path", type=str, required=True)


# [File Summary]
# This script builds the retweet cascades from the collected retweets.
# [Inputs]
# collected_retweets_path:
#   Path to the collected retweets.
#   The collected retweets are saved in the parquet format.
#   This file outputed by collecting_retweets.py.
# qt_ids_path:
#   Path to the quoted tweet ids.
#   exracted by extract_qt_ids.py.
# follow_relation_data_path:
#   Path to the follow relation data.
#   The follow relation data is generated by build_follow_relation_data.py.
# [Outputs]
# save_path:
#   Path to save the social graphs.


def build_social_graphs(row, users, users_ids, quote_ids):
    source_tweet = Tweet(
        tweet_id=row["source_tweet_id"],
        user_id=int(row["source_user_id"]),
        timestamp=row["source_timestamp"],
    )

    retweeted_user_ids = row["retweeted_user_ids"]
    retweeted_user_ids = list(map(int, retweeted_user_ids))
    user_id2tweet_id = dict(zip(
        retweeted_user_ids, row["retweets"]))
    tweet_id2timestamp = dict(
        zip(row["retweets"],
            map(int, row["retweet_timestamps"]))
    )
    retweeted_user_ids = set(retweeted_user_ids)

    G = nx.DiGraph()
    G.add_node(source_tweet.tweet_id, data=source_tweet.model_dump())

    queue = []
    heapq.heappush(queue, source_tweet)

    while queue:
        tweet = heapq.heappop(queue)
        user_id = tweet.user_id
        if user_id not in users_ids:
            continue

        timestamp = tweet.timestamp

        follower_ids = users[user_id]
        follower_ids = [follower_id for follower_id in follower_ids
                        if follower_id in retweeted_user_ids
                        and tweet_id2timestamp[user_id2tweet_id[follower_id]] > timestamp]
        follower_ids = sorted(follower_ids,
                              key=lambda x: tweet_id2timestamp[user_id2tweet_id[x]])
        for follower_id in follower_ids:
            retweet_id = user_id2tweet_id[follower_id]

            if G.has_edge(tweet.tweet_id, retweet_id):
                continue
            if G.has_edge(retweet_id, tweet.tweet_id):
                continue
            if tweet.tweet_id == retweet_id:
                continue

            retweet_timestamp = tweet_id2timestamp[retweet_id]

            retweet = Tweet(
                tweet_id=retweet_id,
                user_id=int(follower_id),
                timestamp=retweet_timestamp,
                is_quote=retweet_id in quote_ids,
            )

            heapq.heappush(queue, retweet)
            retweet_data = retweet.model_dump()

            if not G.has_node(retweet.tweet_id):
                G.add_node(retweet.tweet_id, data=retweet_data)
            G.add_edge(tweet.tweet_id, retweet.tweet_id)

    for node in list(G.nodes):
        in_edges = list(G.in_edges(node)).copy()
        if len(in_edges) <= 1:
            continue

        left_idx = min(list(range(len(in_edges))), key=lambda i: G.nodes[in_edges[i][0]]["data"]["timestamp"])
        in_edges.pop(left_idx)
        G.remove_edges_from(in_edges)

    if nx.is_empty(G):
        return None

    return nx.node_link_data(G)


def main(args):
    df_rt = pl.read_parquet(args.collected_retweets_path)

    df_qt = pl.read_parquet(args.qt_ids_path)
    quote_ids = set(df_qt["tweet_id"].unique().to_list())

    df_follow_relation = pl.read_parquet(args.follow_relation_data_path)

    users_ids = set(df_follow_relation["followee"].unique().to_list())
    users = dict(df_follow_relation.group_by("followee").agg(
        pl.col("follower").alias("follower_ids")
    ).rows())
    df_rt = df_rt.filter(pl.col("cascade_size") > 1)

    with open(args.save_path, "w", encoding="utf-8") as f:
        for row in tqdm(df_rt.rows(named=True), total=len(df_rt)):
            G = build_social_graphs(row, users, users_ids, quote_ids)
            if G is None:
                continue
            line = {
                "source_tweet_id": row["source_tweet_id"],
                "graph": G,
            }
            json.dump(line, f, ensure_ascii=False)
            f.write("\n")


if __name__ == "__main__":
    args = arg_parser.parse_args()
    main(args)
